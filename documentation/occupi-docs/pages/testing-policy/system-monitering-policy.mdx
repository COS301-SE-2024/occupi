# System Monitoring Policy

### 1. Overview
This policy outlines the approach to monitoring the health, performance, and availability of our system. The monitoring strategy includes a combination of tools such as Grafana, Sentry, Loggly, New Relic, and Uptime Robot, each serving a specific purpose in the overall monitoring and alerting system.

### 2. Scope
This policy applies to all services, infrastructure, and applications within the system, including:

Linux nodes (VMs)
MongoDB clusters
Docker containers
Golang backend
APIs and web services

### 3. Monitoring Objectives
The primary objectives of system monitoring are:

To ensure high availability and uptime of services.
To identify and diagnose errors and failures in real-time.
To monitor the performance of key system components.
To provide alerts for any deviations from normal operational parameters.
To collect logs and metrics for auditing and troubleshooting.

### 4. Monitoring Tools and Coverage
#### 4.1 Grafana
Purpose: Real-time monitoring and visualization of system metrics.

Monitored Components:

Linux Nodes: Monitors CPU usage, memory utilization, disk I/O, network bandwidth, and other server performance metrics.
MongoDB Cluster: Tracks MongoDB health, including query performance, replication status, disk usage, and connection pool statistics.
Docker Containers: Monitors container resource usage, including CPU, memory, network, and uptime status.
Metrics Collected:

System resource utilization (CPU, RAM, disk, etc.).
MongoDB query latency, replication lag, and connection pool size.
Docker container health, uptime, and resource consumption.
Alerting: Alerts are set up for any abnormal spikes in resource usage, disk space nearing capacity, or high MongoDB latency.

#### 4.2 Sentry
Purpose: Error tracking and alerting for application-level issues.

Monitored Components:
Golang Backend: Captures application errors, such as internal server errors (500), panics, unhandled exceptions, and other runtime errors.
Errors Tracked:
Internal server errors (HTTP 500).
Application crashes and panics.
Errors with stack traces for quick identification of the root cause.
Alerting: Sentry sends notifications for critical errors that need immediate attention, including high error rates or recurring issues in production.

#### 4.3 Loggly
Purpose: Log aggregation and monitoring.

Monitored Components:
Logrus Logs: Loggly collects and monitors logs generated by the application, including error, warning, info, and debug logs from Logrus in the Golang backend.
Logs Tracked:
API request and response logs.
Errors, warnings, and debugging information from the backend.
Audit logs related to user actions, system changes, and security events.
Alerting: Loggly sends alerts based on predefined log patterns, such as spikes in error logs, unauthorized access attempts, or slow API response times.

#### 4.4 New Relic
Purpose: Application performance monitoring (APM) and detailed tracking of the Golang backend.

Monitored Components:
Golang Backend: Tracks the performance and health of the application, including response times, throughput, and external service interactions (e.g., database queries).
Metrics Collected:
API response times and latency.
Throughput (requests per second).
Database query performance and external service latency.
Resource consumption of the backend (CPU, memory, I/O).
Alerting: New Relic triggers alerts for slow API response times, database performance issues, or increased error rates in backend services.

#### 4.5 Uptime Robot
Purpose: Service uptime monitoring and alerting.

Monitored Components:
Public-Facing APIs and Web Services: Tracks the uptime and availability of key services through HTTP and ping checks.
Metrics Collected:
Service uptime percentage.
Response time for HTTP checks.
Downtime incidents and duration.
Alerting: Uptime Robot sends instant alerts if any of the monitored services become unreachable or experience extended downtime.

### 5. Monitoring and Alerting Workflow
#### 5.1 Monitoring Coverage
Each component (Linux nodes, MongoDB, Docker containers, backend services, and public APIs) is monitored through the designated tools, with the appropriate metrics being collected and visualized.
Alerts are configured based on thresholds that indicate potential issues, such as high CPU usage, application errors, or service downtime.

#### 5.2 Alerts and Notifications
Critical Alerts: Immediate attention is required. Critical alerts are sent via email, Slack, or SMS to the on-call team for issues such as service downtime, database failures, or unhandled application errors.
Warning Alerts: Require investigation but are not service-affecting. Examples include high memory usage or minor increases in API response time.
Info Alerts: Non-critical, informational alerts that indicate minor system changes or routine events (e.g., deployments, low-risk warnings).

#### 5.3 Response and Escalation Process
Alert Detection: When an alert is triggered, the monitoring tool immediately notifies the team through the configured communication channels.
Initial Investigation: The on-call engineer reviews the alert and the corresponding logs/metrics to determine the cause of the issue.
Resolution: Depending on the severity, the issue is resolved immediately (e.g., restarting a service) or escalated to a broader team for deeper investigation.
Post-Incident Review: Critical incidents are followed by a post-incident review to identify the root cause and implement preventive measures.

### 6. Data Retention and Archiving
Grafana: Metrics are retained for 30 days to allow for trend analysis and capacity planning.
Sentry: Error reports are retained for 90 days to ensure sufficient historical data is available for resolving recurring issues.
Loggly: Logs are retained for 6 months to support audit and troubleshooting efforts.
New Relic: Application performance data is retained for 60 days for performance analysis and long-term monitoring.
Uptime Robot: Uptime data is retained for 12 months to support historical availability reports.

### 7. Review and Maintenance
Monitoring configurations, including thresholds and alerting rules, are reviewed quarterly to ensure they are still aligned with system performance and operational goals.
The monitoring infrastructure is regularly tested to ensure it functions correctly during system updates or infrastructure changes.